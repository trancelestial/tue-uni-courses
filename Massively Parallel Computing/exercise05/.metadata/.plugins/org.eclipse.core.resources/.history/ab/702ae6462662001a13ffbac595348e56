#include "Tools.h"

#include <iostream>
#include <iomanip>

using namespace std;

#define VERBOSE // Prints input matrix and results. Only uncomment for small matrix sizes!
#define RUN_CPU // Runs CPU code for reference (slow!!!)
#define N 16 // Must be a multiple of THREADS_PER_BLOCK
#define THREADS_PER_BLOCK 16 // per axis -> block has this value squared threads.
void multiplyMatrix(float* result, const float* a, const float* b, const int n)
{
	for (unsigned int i = 0; i < n; i++)
	{
		for (unsigned int j = 0; j < n; j++)
		{
			result[i * n + j] = 0.0f;
			for (unsigned int k = 0; k < n; k++)
			{
				result[i * n + j] += a[i * n + k] * b[k * n + j];
			}
		}
	}
}

void dumpMatrix(const float* m, const int n)
{
	for (unsigned int i = 0; i < n; i++)
	{
		for (unsigned int j = 0; j < n; j++)
		{
			cout << setw(3) << setprecision(3) << m[i * n + j] << " ";
		}
		cout << endl;
	}
}

float randF(const float min = 0.0f, const float max = 1.0f)
{
	int randI = rand();
	float randF = (float) randI / (float) RAND_MAX;
	float result = min + randF * (max - min);

	return result;
}

__global__ void multiplyMatrixGpu1(float* result, const float* a,
		const float* b, const int n)
{
	// TODO: Implement a trivial GPU square matrix multiplication.
	// Use one thread per output element.
	int tx = blockDim.x*blockIdx.x+threadIdx.x;
	int ty = blockDim.y*blockIdx.y+threadIdx.y;

	float accum = 0;

	for (int k = 0; k < n; k++)
		accum += a[ty*n + k] * b[k*n + tx];

	result[ty*n + tx] = accum;
}

__global__ void multiplyMatrixGpu2(float* result, const float* a,
		const float* b, const int n)
{
	// TODO: Implement a more sophisticated GPU square matrix multiplication.
	// Compute square submatrices per block. Load the common input
	// data of all threads of a block into shared memory cooperatively.
	int bx = blockIdx.x, by = blockIdx.y;
	int tx = threadIdx.x, ty = threadIdx.y;

	int aBegin = by * blockDim.y * n;
	int aEnd = aBegin * blockDim.y - 1;
	int bBegin = bx * blockDim.x;

	int aStep = blockDim.x;
	int bStep = blockDim.x * n;

	float cAccum = 0;

	for (int aIt = aBegin, bIt = bBegin; aIt <= aEnd; aIt += aStep, bIt += bStep) {
		extern __shared__ float aSub[blockDim.x][blockDim.x];
		extern __shared__ float bSub[blockDim.x][blockDim.x];

		aSub[ty][tx] = a[aIt + n*ty + tx];
		bSub[ty][tx] = b[bIt + n*ty + tx];

		__syncthreads();
		//Multiplication
		for (int k = 0; k < blockDim.x; k++)
			cAccum += aSub[ty][k] * bSub[k][tx];
		__syncthreads();
	}
	int c = n * blockDim.x * by + blockDim.x * bx;
	result[c + n * ty + tx] = cAccum;
}

int main(int argc, char **argv)
{
	__int64_t startTime;
	__int64_t endTime;

	// Allocate all memory
	float* hM1 = new float[N * N];
	float* hM2 = new float[N * N];
	float* hMR = new float[N * N];
	float* gM1;
	cudaMalloc(&gM1, sizeof(float) * N * N);
	float* gM2;
	cudaMalloc(&gM2, sizeof(float) * N * N);
	float* gMR;
	cudaMalloc(&gMR, sizeof(float) * N * N);

	// Initialize matrices and upload to CUDA
	for (unsigned int n = 0; n < N * N; n++)
	{
		hM1[n] = randF(-1.0, 1.0);
		hM2[n] = randF(-1.0, 1.0);
	}
	cudaMemcpy(gM1, hM1, sizeof(int) * N * N, cudaMemcpyHostToDevice);
	cudaMemcpy(gM2, hM2, sizeof(int) * N * N, cudaMemcpyHostToDevice);
#ifdef VERBOSE
	cout << "Input Matrices:" << endl;
	dumpMatrix(hM1, N);
	cout << endl;
	dumpMatrix(hM2, N);
	cout << endl << endl;
#endif

#ifdef RUN_CPU
	// Calculations on CPU
	startTime = continuousTimeNs();
	multiplyMatrix(hMR, hM1, hM2, N);
	endTime = continuousTimeNs();
#ifdef VERBOSE
	cout << "CPU:" << endl;
	dumpMatrix(hMR, N);
	cout << endl;
#endif
	cout << "CPU time: " << (endTime - startTime) << "ns" << endl;
#endif

	// Calculations on GPU
	int blocksPerGridX =
			N % THREADS_PER_BLOCK == 0 ?
					N / THREADS_PER_BLOCK : N / THREADS_PER_BLOCK + 1;
	int blocksPerGridY =
			N % THREADS_PER_BLOCK == 0 ?
					N / THREADS_PER_BLOCK : N / THREADS_PER_BLOCK + 1;
	startTime = continuousTimeNs();
	multiplyMatrixGpu1<<<dim3(blocksPerGridX, blocksPerGridY, 1),
			dim3(THREADS_PER_BLOCK, THREADS_PER_BLOCK, 1)>>>(gMR, gM1, gM2, N);
	cudaDeviceSynchronize();
	endTime = continuousTimeNs();
	cudaMemcpy(hMR, gMR, sizeof(float) * N * N, cudaMemcpyDeviceToHost);
#ifdef VERBOSE
	cout << "GPU simple:" << endl;
	dumpMatrix(hMR, N);
	cout << endl;
#endif
	cout << "GPU simple time: " << (endTime - startTime) << "ns" << endl;
	startTime = continuousTimeNs();
	multiplyMatrixGpu2<<<dim3(blocksPerGridX, blocksPerGridY, 1),
			dim3(THREADS_PER_BLOCK, THREADS_PER_BLOCK, 1),
			2*THREADS_PER_BLOCK*THREADS_PER_BLOCK*sizeof(float)>>>(gMR, gM1, gM2, N);
	cudaDeviceSynchronize();
	endTime = continuousTimeNs();
	cudaMemcpy(hMR, gMR, sizeof(float) * N * N, cudaMemcpyDeviceToHost);
#ifdef VERBOSE
	cout << "GPU advanced:" << endl;
	dumpMatrix(hMR, N);
	cout << endl;
#endif
	cout << "GPU advanced time: " << (endTime - startTime) << "ns" << endl;

	// Free all memory
	cudaFree(gM1);
	cudaFree(gM2);
	cudaFree(gMR);
	delete[] hM1;
	delete[] hM2;
	delete[] hMR;
}
