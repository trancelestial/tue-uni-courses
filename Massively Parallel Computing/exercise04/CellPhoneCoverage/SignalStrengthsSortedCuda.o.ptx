//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-26218862
// Cuda compilation tools, release 10.1, V10.1.168
// Based on LLVM 3.4svn
//

.version 6.4
.target sm_75
.address_size 64

.extern .shared .align 4 .b8 partialSum[];
.extern .shared .align 4 .b8 tmp[];

.entry _Z12noSortKernelPK8PositioniPS_P6Bucket(
	.param .u64 _Z12noSortKernelPK8PositioniPS_P6Bucket_param_0,
	.param .u32 _Z12noSortKernelPK8PositioniPS_P6Bucket_param_1,
	.param .u64 _Z12noSortKernelPK8PositioniPS_P6Bucket_param_2,
	.param .u64 _Z12noSortKernelPK8PositioniPS_P6Bucket_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<15>;
	.reg .b32 	%r<99>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd10, [_Z12noSortKernelPK8PositioniPS_P6Bucket_param_0];
	ld.param.u32 	%r33, [_Z12noSortKernelPK8PositioniPS_P6Bucket_param_1];
	ld.param.u64 	%rd11, [_Z12noSortKernelPK8PositioniPS_P6Bucket_param_2];
	ld.param.u64 	%rd9, [_Z12noSortKernelPK8PositioniPS_P6Bucket_param_3];
	cvta.to.global.u64 	%rd1, %rd11;
	cvta.to.global.u64 	%rd2, %rd10;
	setp.lt.s32	%p1, %r33, 1;
	@%p1 bra 	BB0_10;

	and.b32  	%r37, %r33, 3;
	mov.u32 	%r86, 0;
	setp.eq.s32	%p2, %r37, 0;
	@%p2 bra 	BB0_7;

	setp.eq.s32	%p3, %r37, 1;
	@%p3 bra 	BB0_6;

	setp.eq.s32	%p4, %r37, 2;
	@%p4 bra 	BB0_5;

	ld.global.f32 	%f1, [%rd2];
	ld.global.f32 	%f2, [%rd2+4];
	st.global.f32 	[%rd1], %f1;
	st.global.f32 	[%rd1+4], %f2;
	mov.u32 	%r86, 1;

BB0_5:
	mul.wide.u32 	%rd12, %r86, 8;
	add.s64 	%rd13, %rd1, %rd12;
	add.s64 	%rd14, %rd2, %rd12;
	ld.global.f32 	%f3, [%rd14];
	ld.global.f32 	%f4, [%rd14+4];
	st.global.f32 	[%rd13], %f3;
	st.global.f32 	[%rd13+4], %f4;
	add.s32 	%r86, %r86, 1;

BB0_6:
	mul.wide.s32 	%rd15, %r86, 8;
	add.s64 	%rd16, %rd1, %rd15;
	add.s64 	%rd17, %rd2, %rd15;
	ld.global.f32 	%f5, [%rd17];
	ld.global.f32 	%f6, [%rd17+4];
	st.global.f32 	[%rd16], %f5;
	st.global.f32 	[%rd16+4], %f6;
	add.s32 	%r86, %r86, 1;

BB0_7:
	setp.lt.u32	%p5, %r33, 4;
	@%p5 bra 	BB0_10;

	mul.wide.s32 	%rd20, %r86, 8;

BB0_9:
	add.s64 	%rd18, %rd2, %rd20;
	ld.global.f32 	%f7, [%rd18];
	ld.global.f32 	%f8, [%rd18+4];
	add.s64 	%rd19, %rd1, %rd20;
	st.global.f32 	[%rd19], %f7;
	st.global.f32 	[%rd19+4], %f8;
	ld.global.f32 	%f9, [%rd18+8];
	ld.global.f32 	%f10, [%rd18+12];
	st.global.f32 	[%rd19+8], %f9;
	st.global.f32 	[%rd19+12], %f10;
	ld.global.f32 	%f11, [%rd18+16];
	ld.global.f32 	%f12, [%rd18+20];
	st.global.f32 	[%rd19+16], %f11;
	st.global.f32 	[%rd19+20], %f12;
	ld.global.f32 	%f13, [%rd18+24];
	ld.global.f32 	%f14, [%rd18+28];
	st.global.f32 	[%rd19+24], %f13;
	st.global.f32 	[%rd19+28], %f14;
	add.s64 	%rd20, %rd20, 32;
	add.s32 	%r86, %r86, 4;
	setp.lt.s32	%p6, %r86, %r33;
	@%p6 bra 	BB0_9;

BB0_10:
	shl.b32 	%r8, %r33, 3;
	mul.lo.s32 	%r96, %r33, 7;
	mul.lo.s32 	%r95, %r33, 6;
	mul.lo.s32 	%r94, %r33, 5;
	shl.b32 	%r93, %r33, 2;
	mul.lo.s32 	%r92, %r33, 3;
	shl.b32 	%r91, %r33, 1;
	cvta.to.global.u64 	%rd21, %rd9;
	mov.u32 	%r98, -256;
	mov.u32 	%r90, 0;
	mov.u32 	%r97, %r8;

BB0_11:
	shr.s32 	%r41, %r90, 31;
	shr.u32 	%r42, %r41, 24;
	add.s32 	%r43, %r90, %r42;
	shr.s32 	%r44, %r43, 8;
	st.global.u32 	[%rd21], %r44;
	add.s32 	%r45, %r33, %r90;
	shr.s32 	%r46, %r45, 31;
	shr.u32 	%r47, %r46, 24;
	add.s32 	%r48, %r45, %r47;
	shr.s32 	%r49, %r48, 8;
	sub.s32 	%r50, %r49, %r44;
	st.global.u32 	[%rd21+4], %r50;
	st.global.u32 	[%rd21+8], %r49;
	shr.s32 	%r51, %r91, 31;
	shr.u32 	%r52, %r51, 24;
	add.s32 	%r53, %r91, %r52;
	shr.s32 	%r54, %r53, 8;
	sub.s32 	%r55, %r54, %r49;
	st.global.u32 	[%rd21+12], %r55;
	st.global.u32 	[%rd21+16], %r54;
	shr.s32 	%r56, %r92, 31;
	shr.u32 	%r57, %r56, 24;
	add.s32 	%r58, %r92, %r57;
	shr.s32 	%r59, %r58, 8;
	sub.s32 	%r60, %r59, %r54;
	st.global.u32 	[%rd21+20], %r60;
	st.global.u32 	[%rd21+24], %r59;
	shr.s32 	%r61, %r93, 31;
	shr.u32 	%r62, %r61, 24;
	add.s32 	%r63, %r93, %r62;
	shr.s32 	%r64, %r63, 8;
	sub.s32 	%r65, %r64, %r59;
	st.global.u32 	[%rd21+28], %r65;
	st.global.u32 	[%rd21+32], %r64;
	shr.s32 	%r66, %r94, 31;
	shr.u32 	%r67, %r66, 24;
	add.s32 	%r68, %r94, %r67;
	shr.s32 	%r69, %r68, 8;
	sub.s32 	%r70, %r69, %r64;
	st.global.u32 	[%rd21+36], %r70;
	st.global.u32 	[%rd21+40], %r69;
	shr.s32 	%r71, %r95, 31;
	shr.u32 	%r72, %r71, 24;
	add.s32 	%r73, %r95, %r72;
	shr.s32 	%r74, %r73, 8;
	sub.s32 	%r75, %r74, %r69;
	st.global.u32 	[%rd21+44], %r75;
	st.global.u32 	[%rd21+48], %r74;
	shr.s32 	%r76, %r96, 31;
	shr.u32 	%r77, %r76, 24;
	add.s32 	%r78, %r96, %r77;
	shr.s32 	%r79, %r78, 8;
	sub.s32 	%r80, %r79, %r74;
	st.global.u32 	[%rd21+52], %r80;
	st.global.u32 	[%rd21+56], %r79;
	shr.s32 	%r81, %r97, 31;
	shr.u32 	%r82, %r81, 24;
	add.s32 	%r83, %r97, %r82;
	shr.s32 	%r84, %r83, 8;
	sub.s32 	%r85, %r84, %r79;
	st.global.u32 	[%rd21+60], %r85;
	add.s32 	%r97, %r97, %r8;
	add.s64 	%rd21, %rd21, 64;
	add.s32 	%r96, %r96, %r8;
	add.s32 	%r95, %r95, %r8;
	add.s32 	%r94, %r94, %r8;
	add.s32 	%r93, %r93, %r8;
	add.s32 	%r92, %r92, %r8;
	add.s32 	%r91, %r91, %r8;
	add.s32 	%r90, %r90, %r8;
	add.s32 	%r98, %r98, 8;
	setp.ne.s32	%p7, %r98, 0;
	@%p7 bra 	BB0_11;

	ret;
}

.entry _Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc(
	.param .u64 _Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc_param_0,
	.param .u64 _Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc_param_1,
	.param .u32 _Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc_param_2,
	.param .u64 _Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc_param_3,
	.param .u64 _Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd1, [_Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc_param_0];
	ld.param.u64 	%rd2, [_Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc_param_1];
	ld.param.u64 	%rd3, [_Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc_param_3];
	ld.param.u64 	%rd4, [_Z19bucketScatterKernelP8PositionPKS_iPiP9BucketLoc_param_4];
	mov.u32 	%r7, %ntid.x;
	mov.u32 	%r8, %ctaid.x;
	mov.u32 	%r9, %tid.x;
	mad.lo.s32 	%r1, %r7, %r8, %r9;
	cvta.to.global.u64 	%rd5, %rd4;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r2, [%rd7+4];
	ld.global.u32 	%r3, [%rd7];
	setp.eq.s32	%p1, %r3, 0;
	mov.u32 	%r12, 0;
	@%p1 bra 	BB1_2;

	cvta.to.global.u64 	%rd8, %rd3;
	add.s32 	%r10, %r3, -1;
	mul.wide.s32 	%rd9, %r10, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.u32 	%r12, [%rd10];

BB1_2:
	cvta.to.global.u64 	%rd11, %rd1;
	cvta.to.global.u64 	%rd12, %rd2;
	add.s32 	%r11, %r12, %r2;
	mul.wide.s32 	%rd13, %r11, 8;
	add.s64 	%rd14, %rd11, %rd13;
	add.s64 	%rd16, %rd12, %rd6;
	ld.global.f32 	%f1, [%rd16];
	ld.global.f32 	%f2, [%rd16+4];
	st.global.f32 	[%rd14], %f1;
	st.global.f32 	[%rd14+4], %f2;
	ret;
}

.entry _Z20fillBucketInfoKernelP6BucketPi(
	.param .u64 _Z20fillBucketInfoKernelP6BucketPi_param_0,
	.param .u64 _Z20fillBucketInfoKernelP6BucketPi_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd2, [_Z20fillBucketInfoKernelP6BucketPi_param_0];
	ld.param.u64 	%rd3, [_Z20fillBucketInfoKernelP6BucketPi_param_1];
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %ctaid.x;
	mov.u32 	%r7, %tid.x;
	mad.lo.s32 	%r1, %r5, %r6, %r7;
	setp.eq.s32	%p1, %r1, 0;
	mov.u32 	%r10, 0;
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.s32 	%rd5, %r1, 4;
	add.s64 	%rd1, %rd4, %rd5;
	@%p1 bra 	BB2_2;

	ld.global.u32 	%r10, [%rd1+-4];

BB2_2:
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.s32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r10;
	ld.global.u32 	%r8, [%rd1];
	sub.s32 	%r9, %r8, %r10;
	st.global.u32 	[%rd8+4], %r9;
	ret;
}

.entry _Z15histogramKernelPK8PositioniPiffffP9BucketLoc(
	.param .u64 _Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_0,
	.param .u32 _Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_1,
	.param .u64 _Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_2,
	.param .f32 _Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_3,
	.param .f32 _Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_4,
	.param .f32 _Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_5,
	.param .f32 _Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_6,
	.param .u64 _Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_7
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd1, [_Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_0];
	ld.param.u32 	%r2, [_Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_1];
	ld.param.u64 	%rd2, [_Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_2];
	ld.param.u64 	%rd3, [_Z15histogramKernelPK8PositioniPiffffP9BucketLoc_param_7];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB3_2;

	cvta.to.global.u64 	%rd4, %rd2;
	cvta.to.global.u64 	%rd5, %rd1;
	mul.wide.s32 	%rd6, %r1, 8;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.f32 	%f1, [%rd7];
	mul.f32 	%f2, %f1, 0f41800000;
	cvt.rzi.s32.f32	%r6, %f2;
	ld.global.f32 	%f3, [%rd7+4];
	mul.f32 	%f4, %f3, 0f41800000;
	cvt.rzi.s32.f32	%r7, %f4;
	shl.b32 	%r8, %r7, 4;
	add.s32 	%r9, %r8, %r6;
	cvta.to.global.u64 	%rd8, %rd3;
	add.s64 	%rd9, %rd8, %rd6;
	st.global.u32 	[%rd9], %r9;
	mul.wide.u32 	%rd10, %r9, 4;
	add.s64 	%rd11, %rd4, %rd10;
	atom.global.add.u32 	%r10, [%rd11], 1;
	st.global.u32 	[%rd9+4], %r10;

BB3_2:
	ret;
}

	// .globl	_Z12reduceKernelPi
.visible .entry _Z12reduceKernelPi(
	.param .u64 _Z12reduceKernelPi_param_0
)
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [_Z12reduceKernelPi_param_0];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r3, %tid.x;
	mul.wide.u32 	%rd4, %r3, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u32 	%r4, [%rd1];
	shl.b32 	%r5, %r3, 2;
	mov.u32 	%r6, partialSum;
	add.s32 	%r1, %r6, %r5;
	st.shared.u32 	[%r1], %r4;
	add.s32 	%r2, %r3, 1;
	bar.sync 	0;
	and.b32  	%r7, %r2, 1;
	setp.eq.b32	%p1, %r7, 1;
	@%p1 bra 	BB4_2;

	ld.shared.u32 	%r8, [%r1];
	ld.shared.u32 	%r9, [%r1+-4];
	add.s32 	%r10, %r9, %r8;
	st.shared.u32 	[%r1], %r10;

BB4_2:
	bar.sync 	0;
	and.b32  	%r11, %r2, 3;
	setp.ne.s32	%p2, %r11, 0;
	@%p2 bra 	BB4_4;

	ld.shared.u32 	%r12, [%r1];
	ld.shared.u32 	%r13, [%r1+-8];
	add.s32 	%r14, %r13, %r12;
	st.shared.u32 	[%r1], %r14;

BB4_4:
	bar.sync 	0;
	and.b32  	%r15, %r2, 7;
	setp.ne.s32	%p3, %r15, 0;
	@%p3 bra 	BB4_6;

	ld.shared.u32 	%r16, [%r1];
	ld.shared.u32 	%r17, [%r1+-16];
	add.s32 	%r18, %r17, %r16;
	st.shared.u32 	[%r1], %r18;

BB4_6:
	bar.sync 	0;
	and.b32  	%r19, %r2, 15;
	setp.ne.s32	%p4, %r19, 0;
	@%p4 bra 	BB4_8;

	ld.shared.u32 	%r20, [%r1];
	ld.shared.u32 	%r21, [%r1+-32];
	add.s32 	%r22, %r21, %r20;
	st.shared.u32 	[%r1], %r22;

BB4_8:
	bar.sync 	0;
	and.b32  	%r23, %r2, 31;
	setp.ne.s32	%p5, %r23, 0;
	@%p5 bra 	BB4_10;

	ld.shared.u32 	%r24, [%r1];
	ld.shared.u32 	%r25, [%r1+-64];
	add.s32 	%r26, %r25, %r24;
	st.shared.u32 	[%r1], %r26;

BB4_10:
	bar.sync 	0;
	and.b32  	%r27, %r2, 63;
	setp.ne.s32	%p6, %r27, 0;
	@%p6 bra 	BB4_12;

	ld.shared.u32 	%r28, [%r1];
	ld.shared.u32 	%r29, [%r1+-128];
	add.s32 	%r30, %r29, %r28;
	st.shared.u32 	[%r1], %r30;

BB4_12:
	bar.sync 	0;
	and.b32  	%r31, %r2, 127;
	setp.ne.s32	%p7, %r31, 0;
	@%p7 bra 	BB4_14;

	ld.shared.u32 	%r32, [%r1];
	ld.shared.u32 	%r33, [%r1+-256];
	add.s32 	%r34, %r33, %r32;
	st.shared.u32 	[%r1], %r34;

BB4_14:
	bar.sync 	0;
	and.b32  	%r35, %r2, 255;
	setp.ne.s32	%p8, %r35, 0;
	@%p8 bra 	BB4_16;

	ld.shared.u32 	%r36, [%r1];
	ld.shared.u32 	%r37, [%r1+-512];
	add.s32 	%r38, %r37, %r36;
	st.shared.u32 	[%r1], %r38;

BB4_16:
	bar.sync 	0;
	ld.shared.u32 	%r39, [%r1];
	st.global.u32 	[%rd1], %r39;
	ret;
}

	// .globl	_Z12prefixKernelPi
.visible .entry _Z12prefixKernelPi(
	.param .u64 _Z12prefixKernelPi_param_0
)
{
	.reg .pred 	%p<18>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<5>;


	ld.param.u64 	%rd2, [_Z12prefixKernelPi_param_0];
	cvta.to.global.u64 	%rd3, %rd2;
	mov.u32 	%r1, %tid.x;
	mul.wide.u32 	%rd4, %r1, 4;
	add.s64 	%rd1, %rd3, %rd4;
	ld.global.u32 	%r4, [%rd1];
	shl.b32 	%r5, %r1, 2;
	mov.u32 	%r6, partialSum;
	add.s32 	%r2, %r6, %r5;
	st.shared.u32 	[%r2], %r4;
	add.s32 	%r3, %r1, 1;
	bar.sync 	0;
	setp.eq.s32	%p1, %r3, 128;
	@%p1 bra 	BB5_3;

	add.s32 	%r7, %r1, 129;
	and.b32  	%r8, %r7, 255;
	setp.ne.s32	%p2, %r8, 0;
	@%p2 bra 	BB5_3;

	ld.shared.u32 	%r9, [%r2];
	ld.shared.u32 	%r10, [%r2+-512];
	add.s32 	%r11, %r10, %r9;
	st.shared.u32 	[%r2], %r11;

BB5_3:
	bar.sync 	0;
	setp.eq.s32	%p3, %r3, 64;
	@%p3 bra 	BB5_6;

	add.s32 	%r12, %r1, 65;
	and.b32  	%r13, %r12, 127;
	setp.ne.s32	%p4, %r13, 0;
	@%p4 bra 	BB5_6;

	ld.shared.u32 	%r14, [%r2];
	ld.shared.u32 	%r15, [%r2+-256];
	add.s32 	%r16, %r15, %r14;
	st.shared.u32 	[%r2], %r16;

BB5_6:
	bar.sync 	0;
	setp.eq.s32	%p5, %r3, 32;
	@%p5 bra 	BB5_9;

	add.s32 	%r17, %r1, 33;
	and.b32  	%r18, %r17, 63;
	setp.ne.s32	%p6, %r18, 0;
	@%p6 bra 	BB5_9;

	ld.shared.u32 	%r19, [%r2];
	ld.shared.u32 	%r20, [%r2+-128];
	add.s32 	%r21, %r20, %r19;
	st.shared.u32 	[%r2], %r21;

BB5_9:
	bar.sync 	0;
	setp.eq.s32	%p7, %r3, 16;
	@%p7 bra 	BB5_12;

	add.s32 	%r22, %r1, 17;
	and.b32  	%r23, %r22, 31;
	setp.ne.s32	%p8, %r23, 0;
	@%p8 bra 	BB5_12;

	ld.shared.u32 	%r24, [%r2];
	ld.shared.u32 	%r25, [%r2+-64];
	add.s32 	%r26, %r25, %r24;
	st.shared.u32 	[%r2], %r26;

BB5_12:
	bar.sync 	0;
	setp.eq.s32	%p9, %r3, 8;
	@%p9 bra 	BB5_15;

	add.s32 	%r27, %r1, 9;
	and.b32  	%r28, %r27, 15;
	setp.ne.s32	%p10, %r28, 0;
	@%p10 bra 	BB5_15;

	ld.shared.u32 	%r29, [%r2];
	ld.shared.u32 	%r30, [%r2+-32];
	add.s32 	%r31, %r30, %r29;
	st.shared.u32 	[%r2], %r31;

BB5_15:
	bar.sync 	0;
	setp.eq.s32	%p11, %r3, 4;
	@%p11 bra 	BB5_18;

	add.s32 	%r32, %r1, 5;
	and.b32  	%r33, %r32, 7;
	setp.ne.s32	%p12, %r33, 0;
	@%p12 bra 	BB5_18;

	ld.shared.u32 	%r34, [%r2];
	ld.shared.u32 	%r35, [%r2+-16];
	add.s32 	%r36, %r35, %r34;
	st.shared.u32 	[%r2], %r36;

BB5_18:
	bar.sync 	0;
	setp.eq.s32	%p13, %r3, 2;
	@%p13 bra 	BB5_21;

	add.s32 	%r37, %r1, 3;
	and.b32  	%r38, %r37, 3;
	setp.ne.s32	%p14, %r38, 0;
	@%p14 bra 	BB5_21;

	ld.shared.u32 	%r39, [%r2];
	ld.shared.u32 	%r40, [%r2+-8];
	add.s32 	%r41, %r40, %r39;
	st.shared.u32 	[%r2], %r41;

BB5_21:
	bar.sync 	0;
	and.b32  	%r42, %r1, 1;
	setp.eq.b32	%p15, %r42, 1;
	setp.eq.s32	%p16, %r1, 0;
	or.pred  	%p17, %p16, %p15;
	@%p17 bra 	BB5_23;

	ld.shared.u32 	%r43, [%r2];
	ld.shared.u32 	%r44, [%r2+-4];
	add.s32 	%r45, %r44, %r43;
	st.shared.u32 	[%r2], %r45;

BB5_23:
	bar.sync 	0;
	ld.shared.u32 	%r46, [%r2];
	st.global.u32 	[%rd1], %r46;
	ret;
}

.entry _Z10xmaxKernelPfPK8Positioni(
	.param .u64 _Z10xmaxKernelPfPK8Positioni_param_0,
	.param .u64 _Z10xmaxKernelPfPK8Positioni_param_1,
	.param .u32 _Z10xmaxKernelPfPK8Positioni_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z10xmaxKernelPfPK8Positioni_param_0];
	ld.param.u64 	%rd2, [_Z10xmaxKernelPfPK8Positioni_param_1];
	ld.param.u32 	%r11, [_Z10xmaxKernelPfPK8Positioni_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.u32	%p1, %r4, %r11;
	@%p1 bra 	BB6_11;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r4, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	shl.b32 	%r12, %r3, 2;
	mov.u32 	%r13, tmp;
	add.s32 	%r5, %r13, %r12;
	st.shared.f32 	[%r5], %f1;
	shr.u32 	%r29, %r2, 1;
	shl.b32 	%r14, %r29, 1;
	setp.ge.u32	%p2, %r14, %r2;
	@%p2 bra 	BB6_5;

	add.s32 	%r15, %r29, %r3;
	shl.b32 	%r16, %r15, 2;
	add.s32 	%r18, %r13, %r16;
	ld.shared.f32 	%f2, [%r18];
	setp.geu.f32	%p3, %f1, %f2;
	@%p3 bra 	BB6_4;

	st.shared.f32 	[%r5], %f2;

BB6_4:
	shl.b32 	%r19, %r2, 2;
	add.s32 	%r21, %r19, %r13;
	ld.shared.f32 	%f4, [%r21+-4];
	shl.b32 	%r22, %r29, 2;
	add.s32 	%r23, %r13, %r22;
	st.shared.f32 	[%r23+4], %f4;
	shr.u32 	%r29, %r2, 2;

BB6_5:
	setp.eq.s32	%p4, %r29, 0;
	@%p4 bra 	BB6_10;

BB6_6:
	bar.sync 	0;
	setp.ge.u32	%p5, %r3, %r29;
	@%p5 bra 	BB6_9;

	ld.shared.f32 	%f5, [%r5];
	add.s32 	%r24, %r29, %r3;
	shl.b32 	%r25, %r24, 2;
	add.s32 	%r27, %r13, %r25;
	ld.shared.f32 	%f3, [%r27];
	setp.geu.f32	%p6, %f5, %f3;
	@%p6 bra 	BB6_9;

	st.shared.f32 	[%r5], %f3;

BB6_9:
	shr.s32 	%r29, %r29, 1;
	setp.gt.s32	%p7, %r29, 0;
	@%p7 bra 	BB6_6;

BB6_10:
	bar.sync 	0;
	ld.shared.f32 	%f6, [tmp];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f6;

BB6_11:
	ret;
}

.entry _Z10xminKernelPfPK8Positioni(
	.param .u64 _Z10xminKernelPfPK8Positioni_param_0,
	.param .u64 _Z10xminKernelPfPK8Positioni_param_1,
	.param .u32 _Z10xminKernelPfPK8Positioni_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z10xminKernelPfPK8Positioni_param_0];
	ld.param.u64 	%rd2, [_Z10xminKernelPfPK8Positioni_param_1];
	ld.param.u32 	%r9, [_Z10xminKernelPfPK8Positioni_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.u32	%p1, %r4, %r9;
	@%p1 bra 	BB7_7;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r4, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	shl.b32 	%r10, %r3, 2;
	mov.u32 	%r11, tmp;
	add.s32 	%r5, %r11, %r10;
	st.shared.f32 	[%r5], %f2;
	shr.u32 	%r16, %r2, 1;
	setp.eq.s32	%p2, %r16, 0;
	@%p2 bra 	BB7_6;

BB7_2:
	bar.sync 	0;
	setp.ge.u32	%p3, %r3, %r16;
	@%p3 bra 	BB7_5;

	ld.shared.f32 	%f3, [%r5];
	add.s32 	%r12, %r16, %r3;
	shl.b32 	%r13, %r12, 2;
	add.s32 	%r15, %r11, %r13;
	ld.shared.f32 	%f1, [%r15];
	setp.leu.f32	%p4, %f3, %f1;
	@%p4 bra 	BB7_5;

	st.shared.f32 	[%r5], %f1;

BB7_5:
	shr.u32 	%r16, %r16, 1;
	setp.ne.s32	%p5, %r16, 0;
	@%p5 bra 	BB7_2;

BB7_6:
	bar.sync 	0;
	ld.shared.f32 	%f4, [tmp];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f4;

BB7_7:
	ret;
}

.entry _Z10ymaxKernelPfPK8Positioni(
	.param .u64 _Z10ymaxKernelPfPK8Positioni_param_0,
	.param .u64 _Z10ymaxKernelPfPK8Positioni_param_1,
	.param .u32 _Z10ymaxKernelPfPK8Positioni_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .f32 	%f<7>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z10ymaxKernelPfPK8Positioni_param_0];
	ld.param.u64 	%rd2, [_Z10ymaxKernelPfPK8Positioni_param_1];
	ld.param.u32 	%r11, [_Z10ymaxKernelPfPK8Positioni_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.u32	%p1, %r4, %r11;
	@%p1 bra 	BB8_11;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r4, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5+4];
	shl.b32 	%r12, %r3, 2;
	mov.u32 	%r13, tmp;
	add.s32 	%r5, %r13, %r12;
	st.shared.f32 	[%r5], %f1;
	shr.u32 	%r29, %r2, 1;
	shl.b32 	%r14, %r29, 1;
	setp.ge.u32	%p2, %r14, %r2;
	@%p2 bra 	BB8_5;

	add.s32 	%r15, %r29, %r3;
	shl.b32 	%r16, %r15, 2;
	add.s32 	%r18, %r13, %r16;
	ld.shared.f32 	%f2, [%r18];
	setp.geu.f32	%p3, %f1, %f2;
	@%p3 bra 	BB8_4;

	st.shared.f32 	[%r5], %f2;

BB8_4:
	shl.b32 	%r19, %r2, 2;
	add.s32 	%r21, %r19, %r13;
	ld.shared.f32 	%f4, [%r21+-4];
	shl.b32 	%r22, %r29, 2;
	add.s32 	%r23, %r13, %r22;
	st.shared.f32 	[%r23+4], %f4;
	shr.u32 	%r29, %r2, 2;

BB8_5:
	setp.eq.s32	%p4, %r29, 0;
	@%p4 bra 	BB8_10;

BB8_6:
	bar.sync 	0;
	setp.ge.u32	%p5, %r3, %r29;
	@%p5 bra 	BB8_9;

	ld.shared.f32 	%f5, [%r5];
	add.s32 	%r24, %r29, %r3;
	shl.b32 	%r25, %r24, 2;
	add.s32 	%r27, %r13, %r25;
	ld.shared.f32 	%f3, [%r27];
	setp.geu.f32	%p6, %f5, %f3;
	@%p6 bra 	BB8_9;

	st.shared.f32 	[%r5], %f3;

BB8_9:
	shr.s32 	%r29, %r29, 1;
	setp.gt.s32	%p7, %r29, 0;
	@%p7 bra 	BB8_6;

BB8_10:
	bar.sync 	0;
	ld.shared.f32 	%f6, [tmp];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f6;

BB8_11:
	ret;
}

.entry _Z10yminKernelPfPK8Positioni(
	.param .u64 _Z10yminKernelPfPK8Positioni_param_0,
	.param .u64 _Z10yminKernelPfPK8Positioni_param_1,
	.param .u32 _Z10yminKernelPfPK8Positioni_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z10yminKernelPfPK8Positioni_param_0];
	ld.param.u64 	%rd2, [_Z10yminKernelPfPK8Positioni_param_1];
	ld.param.u32 	%r9, [_Z10yminKernelPfPK8Positioni_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.u32	%p1, %r4, %r9;
	@%p1 bra 	BB9_7;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r4, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5+4];
	shl.b32 	%r10, %r3, 2;
	mov.u32 	%r11, tmp;
	add.s32 	%r5, %r11, %r10;
	st.shared.f32 	[%r5], %f2;
	shr.u32 	%r16, %r2, 1;
	setp.eq.s32	%p2, %r16, 0;
	@%p2 bra 	BB9_6;

BB9_2:
	bar.sync 	0;
	setp.ge.u32	%p3, %r3, %r16;
	@%p3 bra 	BB9_5;

	ld.shared.f32 	%f3, [%r5];
	add.s32 	%r12, %r16, %r3;
	shl.b32 	%r13, %r12, 2;
	add.s32 	%r15, %r11, %r13;
	ld.shared.f32 	%f1, [%r15];
	setp.leu.f32	%p4, %f3, %f1;
	@%p4 bra 	BB9_5;

	st.shared.f32 	[%r5], %f1;

BB9_5:
	shr.u32 	%r16, %r16, 1;
	setp.ne.s32	%p5, %r16, 0;
	@%p5 bra 	BB9_2;

BB9_6:
	bar.sync 	0;
	ld.shared.f32 	%f4, [tmp];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f4;

BB9_7:
	ret;
}

.entry _Z15maxKernelReducePfPKfi(
	.param .u64 _Z15maxKernelReducePfPKfi_param_0,
	.param .u64 _Z15maxKernelReducePfPKfi_param_1,
	.param .u32 _Z15maxKernelReducePfPKfi_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z15maxKernelReducePfPKfi_param_0];
	ld.param.u64 	%rd2, [_Z15maxKernelReducePfPKfi_param_1];
	ld.param.u32 	%r9, [_Z15maxKernelReducePfPKfi_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.u32	%p1, %r4, %r9;
	@%p1 bra 	BB10_7;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r4, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	shl.b32 	%r10, %r3, 2;
	mov.u32 	%r11, tmp;
	add.s32 	%r5, %r11, %r10;
	st.shared.f32 	[%r5], %f2;
	shr.u32 	%r16, %r2, 1;
	setp.eq.s32	%p2, %r16, 0;
	@%p2 bra 	BB10_6;

BB10_2:
	bar.sync 	0;
	setp.ge.u32	%p3, %r3, %r16;
	@%p3 bra 	BB10_5;

	ld.shared.f32 	%f3, [%r5];
	add.s32 	%r12, %r16, %r3;
	shl.b32 	%r13, %r12, 2;
	add.s32 	%r15, %r11, %r13;
	ld.shared.f32 	%f1, [%r15];
	setp.geu.f32	%p4, %f3, %f1;
	@%p4 bra 	BB10_5;

	st.shared.f32 	[%r5], %f1;

BB10_5:
	shr.u32 	%r16, %r16, 1;
	setp.ne.s32	%p5, %r16, 0;
	@%p5 bra 	BB10_2;

BB10_6:
	bar.sync 	0;
	ld.shared.f32 	%f4, [tmp];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f4;

BB10_7:
	ret;
}

.entry _Z15minKernelReducePfPKfi(
	.param .u64 _Z15minKernelReducePfPKfi_param_0,
	.param .u64 _Z15minKernelReducePfPKfi_param_1,
	.param .u32 _Z15minKernelReducePfPKfi_param_2
)
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<5>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_Z15minKernelReducePfPKfi_param_0];
	ld.param.u64 	%rd2, [_Z15minKernelReducePfPKfi_param_1];
	ld.param.u32 	%r9, [_Z15minKernelReducePfPKfi_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.u32	%p1, %r4, %r9;
	@%p1 bra 	BB11_7;

	cvta.to.global.u64 	%rd3, %rd2;
	mul.wide.u32 	%rd4, %r4, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f2, [%rd5];
	shl.b32 	%r10, %r3, 2;
	mov.u32 	%r11, tmp;
	add.s32 	%r5, %r11, %r10;
	st.shared.f32 	[%r5], %f2;
	shr.u32 	%r16, %r2, 1;
	setp.eq.s32	%p2, %r16, 0;
	@%p2 bra 	BB11_6;

BB11_2:
	bar.sync 	0;
	setp.ge.u32	%p3, %r3, %r16;
	@%p3 bra 	BB11_5;

	ld.shared.f32 	%f3, [%r5];
	add.s32 	%r12, %r16, %r3;
	shl.b32 	%r13, %r12, 2;
	add.s32 	%r15, %r11, %r13;
	ld.shared.f32 	%f1, [%r15];
	setp.leu.f32	%p4, %f3, %f1;
	@%p4 bra 	BB11_5;

	st.shared.f32 	[%r5], %f1;

BB11_5:
	shr.u32 	%r16, %r16, 1;
	setp.ne.s32	%p5, %r16, 0;
	@%p5 bra 	BB11_2;

BB11_6:
	bar.sync 	0;
	ld.shared.f32 	%f4, [tmp];
	cvta.to.global.u64 	%rd6, %rd1;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f4;

BB11_7:
	ret;
}

.entry _Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf(
	.param .u64 _Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_0,
	.param .u64 _Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_1,
	.param .u64 _Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_2,
	.param .u64 _Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_3,
	.param .u64 _Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_4
)
{
	.reg .pred 	%p<40>;
	.reg .f32 	%f<117>;
	.reg .b32 	%r<47>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<31>;


	ld.param.u64 	%rd14, [_Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_0];
	ld.param.u64 	%rd11, [_Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_1];
	ld.param.u64 	%rd12, [_Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_2];
	ld.param.u64 	%rd15, [_Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_3];
	ld.param.u64 	%rd13, [_Z36calculateSignalStrengthsSortedKernelPK8PositionPK6BucketS1_S4_Pf_param_4];
	cvta.to.global.u64 	%rd1, %rd14;
	mov.u32 	%r1, %ctaid.y;
	shl.b32 	%r29, %r1, 4;
	mov.u32 	%r2, %ctaid.x;
	add.s32 	%r30, %r29, %r2;
	cvta.to.global.u64 	%rd16, %rd15;
	mul.wide.s32 	%rd17, %r30, 8;
	add.s64 	%rd2, %rd16, %rd17;
	mov.u32 	%r40, %tid.x;
	ld.global.u32 	%r4, [%rd2+4];
	setp.ge.s32	%p1, %r40, %r4;
	@%p1 bra 	BB12_21;

	cvta.to.global.u64 	%rd3, %rd12;
	cvta.to.global.u64 	%rd4, %rd11;
	cvta.to.global.u64 	%rd5, %rd13;
	ld.global.u32 	%r5, [%rd2];
	add.s32 	%r6, %r1, -1;
	add.s32 	%r7, %r1, 2;
	mov.u32 	%r8, %ntid.x;
	add.s32 	%r9, %r2, -1;
	add.s32 	%r10, %r2, 2;

BB12_2:
	add.s32 	%r12, %r40, %r5;
	mov.f32 	%f114, 0f00000000;
	setp.ge.s32	%p2, %r6, %r7;
	@%p2 bra 	BB12_20;

	mul.wide.s32 	%rd18, %r12, 8;
	add.s64 	%rd6, %rd3, %rd18;
	mov.f32 	%f114, 0f00000000;
	mov.u32 	%r41, %r6;

BB12_4:
	setp.ge.s32	%p3, %r9, %r10;
	@%p3 bra 	BB12_19;

	shl.b32 	%r14, %r41, 4;
	setp.lt.s32	%p4, %r41, 0;
	mov.u32 	%r42, %r9;
	@%p4 bra 	BB12_19;

BB12_6:
	setp.gt.u32	%p5, %r42, 15;
	setp.lt.s32	%p6, %r42, 0;
	or.pred  	%p7, %p6, %p5;
	setp.gt.u32	%p8, %r41, 15;
	or.pred  	%p9, %p7, %p8;
	@%p9 bra 	BB12_18;

	add.s32 	%r31, %r42, %r14;
	mul.wide.s32 	%rd19, %r31, 8;
	add.s64 	%rd20, %rd4, %rd19;
	ld.global.u32 	%r16, [%rd20];
	cvt.s64.s32	%rd7, %r16;
	ld.global.u32 	%r17, [%rd20+4];
	setp.lt.s32	%p10, %r17, 1;
	@%p10 bra 	BB12_18;

	ld.global.f32 	%f3, [%rd6];
	ld.global.f32 	%f4, [%rd6+4];
	and.b32  	%r18, %r17, 3;
	setp.eq.s32	%p11, %r18, 0;
	mov.f32 	%f19, 0f00000000;
	mov.u32 	%r46, 0;
	@%p11 bra 	BB12_9;
	bra.uni 	BB12_10;

BB12_9:
	mov.f32 	%f111, %f114;
	mov.f32 	%f114, %f19;
	bra.uni 	BB12_15;

BB12_10:
	setp.eq.s32	%p12, %r18, 1;
	mov.u32 	%r44, 0;
	@%p12 bra 	BB12_14;

	setp.eq.s32	%p13, %r18, 2;
	mov.u32 	%r43, 0;
	@%p13 bra 	BB12_13;

	mul.wide.s32 	%rd21, %r16, 8;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.f32 	%f20, [%rd22];
	sub.f32 	%f21, %f3, %f20;
	ld.global.f32 	%f22, [%rd22+4];
	sub.f32 	%f23, %f4, %f22;
	mul.f32 	%f24, %f23, %f23;
	fma.rn.f32 	%f25, %f21, %f21, %f24;
	sqrt.rn.f32 	%f26, %f25;
	cvt.f64.f32	%fd1, %f26;
	setp.lt.f64	%p14, %fd1, 0d3DDB7CDFD9D7BDBB;
	selp.f32	%f27, 0f2EDBE6FF, %f26, %p14;
	mul.f32 	%f28, %f27, %f27;
	rcp.rn.f32 	%f29, %f28;
	mul.f32 	%f30, %f29, 0f399D4953;
	setp.lt.f32	%p15, %f30, 0f3F800000;
	selp.f32	%f31, 0f00000000, %f30, %p15;
	setp.lt.f32	%p16, %f114, %f31;
	selp.f32	%f114, %f31, %f114, %p16;
	mov.u32 	%r43, 1;

BB12_13:
	add.s32 	%r36, %r43, %r16;
	mul.wide.s32 	%rd23, %r36, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.f32 	%f32, [%rd24];
	sub.f32 	%f33, %f3, %f32;
	ld.global.f32 	%f34, [%rd24+4];
	sub.f32 	%f35, %f4, %f34;
	mul.f32 	%f36, %f35, %f35;
	fma.rn.f32 	%f37, %f33, %f33, %f36;
	sqrt.rn.f32 	%f38, %f37;
	cvt.f64.f32	%fd2, %f38;
	setp.lt.f64	%p17, %fd2, 0d3DDB7CDFD9D7BDBB;
	selp.f32	%f39, 0f2EDBE6FF, %f38, %p17;
	mul.f32 	%f40, %f39, %f39;
	rcp.rn.f32 	%f41, %f40;
	mul.f32 	%f42, %f41, 0f399D4953;
	setp.lt.f32	%p18, %f42, 0f3F800000;
	selp.f32	%f43, 0f00000000, %f42, %p18;
	setp.lt.f32	%p19, %f114, %f43;
	selp.f32	%f114, %f43, %f114, %p19;
	add.s32 	%r44, %r43, 1;

BB12_14:
	add.s32 	%r37, %r44, %r16;
	mul.wide.s32 	%rd25, %r37, 8;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.f32 	%f44, [%rd26];
	sub.f32 	%f45, %f3, %f44;
	ld.global.f32 	%f46, [%rd26+4];
	sub.f32 	%f47, %f4, %f46;
	mul.f32 	%f48, %f47, %f47;
	fma.rn.f32 	%f49, %f45, %f45, %f48;
	sqrt.rn.f32 	%f50, %f49;
	cvt.f64.f32	%fd3, %f50;
	setp.lt.f64	%p20, %fd3, 0d3DDB7CDFD9D7BDBB;
	selp.f32	%f51, 0f2EDBE6FF, %f50, %p20;
	mul.f32 	%f52, %f51, %f51;
	rcp.rn.f32 	%f53, %f52;
	mul.f32 	%f54, %f53, 0f399D4953;
	setp.lt.f32	%p21, %f54, 0f3F800000;
	selp.f32	%f55, 0f00000000, %f54, %p21;
	setp.lt.f32	%p22, %f114, %f55;
	selp.f32	%f111, %f55, %f114, %p22;
	add.s32 	%r46, %r44, 1;
	mov.f32 	%f114, %f111;

BB12_15:
	setp.lt.u32	%p23, %r17, 4;
	@%p23 bra 	BB12_18;

	cvt.u32.u64	%r38, %rd7;
	add.s32 	%r39, %r46, %r38;
	mul.wide.s32 	%rd27, %r39, 8;
	add.s64 	%rd30, %rd1, %rd27;
	mov.f32 	%f114, %f111;

BB12_17:
	ld.global.f32 	%f56, [%rd30];
	sub.f32 	%f57, %f3, %f56;
	ld.global.f32 	%f58, [%rd30+4];
	sub.f32 	%f59, %f4, %f58;
	mul.f32 	%f60, %f59, %f59;
	fma.rn.f32 	%f61, %f57, %f57, %f60;
	sqrt.rn.f32 	%f62, %f61;
	cvt.f64.f32	%fd4, %f62;
	setp.lt.f64	%p24, %fd4, 0d3DDB7CDFD9D7BDBB;
	selp.f32	%f63, 0f2EDBE6FF, %f62, %p24;
	mul.f32 	%f64, %f63, %f63;
	rcp.rn.f32 	%f65, %f64;
	mul.f32 	%f66, %f65, 0f399D4953;
	setp.lt.f32	%p25, %f66, 0f3F800000;
	selp.f32	%f67, 0f00000000, %f66, %p25;
	setp.lt.f32	%p26, %f114, %f67;
	selp.f32	%f68, %f67, %f114, %p26;
	ld.global.f32 	%f69, [%rd30+8];
	sub.f32 	%f70, %f3, %f69;
	ld.global.f32 	%f71, [%rd30+12];
	sub.f32 	%f72, %f4, %f71;
	mul.f32 	%f73, %f72, %f72;
	fma.rn.f32 	%f74, %f70, %f70, %f73;
	sqrt.rn.f32 	%f75, %f74;
	cvt.f64.f32	%fd5, %f75;
	setp.lt.f64	%p27, %fd5, 0d3DDB7CDFD9D7BDBB;
	selp.f32	%f76, 0f2EDBE6FF, %f75, %p27;
	mul.f32 	%f77, %f76, %f76;
	rcp.rn.f32 	%f78, %f77;
	mul.f32 	%f79, %f78, 0f399D4953;
	setp.lt.f32	%p28, %f79, 0f3F800000;
	selp.f32	%f80, 0f00000000, %f79, %p28;
	setp.lt.f32	%p29, %f68, %f80;
	selp.f32	%f81, %f80, %f68, %p29;
	ld.global.f32 	%f82, [%rd30+16];
	sub.f32 	%f83, %f3, %f82;
	ld.global.f32 	%f84, [%rd30+20];
	sub.f32 	%f85, %f4, %f84;
	mul.f32 	%f86, %f85, %f85;
	fma.rn.f32 	%f87, %f83, %f83, %f86;
	sqrt.rn.f32 	%f88, %f87;
	cvt.f64.f32	%fd6, %f88;
	setp.lt.f64	%p30, %fd6, 0d3DDB7CDFD9D7BDBB;
	selp.f32	%f89, 0f2EDBE6FF, %f88, %p30;
	mul.f32 	%f90, %f89, %f89;
	rcp.rn.f32 	%f91, %f90;
	mul.f32 	%f92, %f91, 0f399D4953;
	setp.lt.f32	%p31, %f92, 0f3F800000;
	selp.f32	%f93, 0f00000000, %f92, %p31;
	setp.lt.f32	%p32, %f81, %f93;
	selp.f32	%f94, %f93, %f81, %p32;
	ld.global.f32 	%f95, [%rd30+24];
	sub.f32 	%f96, %f3, %f95;
	ld.global.f32 	%f97, [%rd30+28];
	sub.f32 	%f98, %f4, %f97;
	mul.f32 	%f99, %f98, %f98;
	fma.rn.f32 	%f100, %f96, %f96, %f99;
	sqrt.rn.f32 	%f101, %f100;
	cvt.f64.f32	%fd7, %f101;
	setp.lt.f64	%p33, %fd7, 0d3DDB7CDFD9D7BDBB;
	selp.f32	%f102, 0f2EDBE6FF, %f101, %p33;
	mul.f32 	%f103, %f102, %f102;
	rcp.rn.f32 	%f104, %f103;
	mul.f32 	%f105, %f104, 0f399D4953;
	setp.lt.f32	%p34, %f105, 0f3F800000;
	selp.f32	%f106, 0f00000000, %f105, %p34;
	setp.lt.f32	%p35, %f94, %f106;
	selp.f32	%f114, %f106, %f94, %p35;
	add.s64 	%rd30, %rd30, 32;
	add.s32 	%r46, %r46, 4;
	setp.lt.s32	%p36, %r46, %r17;
	@%p36 bra 	BB12_17;

BB12_18:
	add.s32 	%r42, %r42, 1;
	setp.lt.s32	%p37, %r42, %r10;
	@%p37 bra 	BB12_6;

BB12_19:
	add.s32 	%r41, %r41, 1;
	setp.lt.s32	%p38, %r41, %r7;
	@%p38 bra 	BB12_4;

BB12_20:
	mul.wide.s32 	%rd28, %r12, 4;
	add.s64 	%rd29, %rd5, %rd28;
	st.global.f32 	[%rd29], %f114;
	add.s32 	%r40, %r8, %r40;
	setp.lt.s32	%p39, %r40, %r4;
	@%p39 bra 	BB12_2;

BB12_21:
	ret;
}


